{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 - Neural net foudnations\n",
    "\n",
    "[FAQ, resources and official course updates](https://forums.fast.ai/t/faq-resources-and-official-course-updates/95292)\n",
    "\n",
    "Lecture summary:\n",
    "\n",
    "* Recommend to watch lecture 0 for \"how to fast.ai\". \n",
    "\n",
    "* Review of student work.\n",
    "\n",
    "* Introduction of Paperspace Gradient\n",
    "\n",
    "   - Note thatI use this, it is not really the best anymore since Digital Ocean bought them though. [Downtime](https://status.paperspace.com/) is more frequent then i would like, and machine startup can be SLOW.\n",
    "   - Best feature is the ability to use the remote desktop.\n",
    "   - You can use jupyter lab through there.\n",
    "\n",
    "* Improving pet classifier by using other models- `timm` module has many models you can try. If you use timm, just use the model name as a string in `vision_learner`.\n",
    "\n",
    "* fast.ai stores categories in `dls.vocab` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): AdaptiveConcatPool2d(\n",
       "      (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "      (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "    )\n",
       "    (1): fastai.layers.Flatten(full=False)\n",
       "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "    (4): Linear(in_features=1024, out_features=512, bias=False)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.5, inplace=False)\n",
       "    (8): Linear(in_features=512, out_features=2, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "# need the is cat function!\n",
    "def is_cat(x): return x[0].isupper()\n",
    "\n",
    "learn = load_learner('model.pkl')\n",
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers upon layers ! *Deep* learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = learn.model.get_submodule('0.1')   # this is a BatchNorm2d layer\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([ 2.3446e-01,  2.6621e-01, -5.1096e-08,  5.1795e-01,  3.4404e-09,\n",
       "          2.2224e-01,  4.2251e-01,  1.3153e-07,  2.5180e-01,  1.5152e-06,\n",
       "          3.1656e-01,  2.4947e-01,  3.7797e-01,  1.0862e-05,  2.7471e-01,\n",
       "          2.3831e-01,  2.4306e-01,  3.9593e-01,  4.7155e-01,  2.9069e-01,\n",
       "          2.7300e-01,  2.7844e-01,  2.9069e-01,  2.0527e-01,  2.6012e-01,\n",
       "          2.7987e-01,  2.9207e-01,  3.1485e-01,  3.8960e-01,  3.0314e-01,\n",
       "          2.6662e-01,  2.1045e-01,  2.8660e-01,  3.3190e-01,  4.2864e-01,\n",
       "          3.7165e-01,  7.4804e-08,  1.9039e-01,  1.4740e-08,  2.2395e-01,\n",
       "          1.8058e-01,  2.4862e-01,  2.7337e-01,  2.5842e-01,  2.9521e-01,\n",
       "          2.9983e-01,  2.2335e-01,  2.6272e-01,  2.2001e-08,  2.6464e-01,\n",
       "          2.1989e-01,  2.8322e-01,  3.2972e-01,  2.2612e-01,  3.6701e-01,\n",
       "          2.1193e-01,  2.3936e-01,  2.5123e-01,  5.2742e-01,  2.4820e-01,\n",
       "          2.9512e-01,  2.5859e-01,  4.8475e-01,  2.6641e-01],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 2.3162e-01,  2.5306e-01, -1.0543e-06, -6.6509e-01, -1.6571e-08,\n",
       "          1.6057e-01,  4.5493e-01, -4.3020e-07,  2.9978e-01, -8.0052e-06,\n",
       "          3.4966e-01,  3.1354e-01, -2.5021e-01, -3.4749e-05,  1.0840e-01,\n",
       "          2.1928e-01,  3.8262e-01, -5.2923e-01, -6.2612e-01,  5.7086e-01,\n",
       "          3.0027e-01,  5.8529e-01,  4.8185e-01,  3.2813e-01,  1.9664e-01,\n",
       "          1.9480e-01,  1.5265e-01,  8.4301e-02,  5.1421e-01,  1.3861e-02,\n",
       "          1.6650e-01,  3.3102e-01,  2.4641e-01,  4.4381e-01, -2.7856e-01,\n",
       "         -2.1811e-02, -2.4507e-07,  3.2094e-01, -4.9152e-08,  2.3846e-01,\n",
       "          2.3312e-01,  3.1441e-01,  4.2925e-01,  2.9256e-01,  2.6245e-01,\n",
       "          6.7514e-01,  4.2996e-01,  3.4621e-01, -8.6909e-08,  2.4790e-01,\n",
       "          3.0337e-01,  6.1603e-01,  3.9701e-01,  3.3093e-01, -4.1066e-01,\n",
       "          3.7872e-01,  1.7792e-01,  2.5802e-01, -4.4805e-01,  2.1384e-01,\n",
       "          5.6755e-01,  5.7207e-01, -4.0183e-01,  2.3411e-01],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(l.parameters())  # parameters is a generator, so we convert it to a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does a neural network really work?\n",
    "[Video: 23.51](https://youtu.be/hBBOjCiFcuo?t=1431)\n",
    "\n",
    "[Kaggle notebook](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work)\n",
    "\n",
    "Aside: `@interact` is really cool!   [Documentation](https://ipywidgets.readthedocs.io/en/latest/examples/Using%20Interact.html)\n",
    "\n",
    "The notebook above shows how gradient descent can help fit a function (using torch autograd) and also discusses the [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "def plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n",
    "    x = torch.linspace(min,max, 100)[:,None]\n",
    "    if ylim: plt.ylim(ylim)\n",
    "    plt.plot(x, f(x), color)\n",
    "    if title is not None: plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def rectified_linear(m,b,x): return F.relu(m*x+b)\n",
    "\n",
    "def triple_relu(m1,b1,m2,b2,m3,b3,x):\n",
    "\n",
    "\n",
    "    return rectified_linear(m3,b3,x)+rectified_linear(m2,b2,x) +  rectified_linear(m1,b1,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad9bec0f4f44315a9ebb11b5c2e33a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-1.5, description='m1', max=1.5, min=-4.5), FloatSlider(value=0.6, desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(m1=-1.5, b1=0.6, m2=1.5, b2=2.2,  m3 =0.4, b3 =0.6)\n",
    "def plot_triple_relu(m1, b1, m2, b2, m3, b3):\n",
    "    plot_function(partial(triple_relu, m1,b1,m2,b2,m3,b3), ylim=(-1,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Aside \n",
    "   \n",
    "   Note that we could also vary the coefficients in front of each term as well as add an offset, \n",
    "   which is needed to get full flexibility and is what a simple single hidden layer neural network would look like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.5493],\n",
       "         [-0.0184],\n",
       "         [-0.5076]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1809, -0.2949,  0.5674], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.2684,  0.0693, -0.0867]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.4008], requires_grad=True)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple single hidden layer network with 3 hidden units:\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(1,3)\n",
    "        self.output = nn.Linear(3,1)\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "    \n",
    "model = NN()\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.5493],\n",
       "         [-0.0184],\n",
       "         [-0.5076]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1809, -0.2949,  0.5674], requires_grad=True)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model.hidden.weight, model.hidden.bias]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the hidden layer are the three `m` and `b` variables we had before, but we can combine the three units in any linear combination, which are the paramters of the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.2684,  0.0693, -0.0867]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.4008], requires_grad=True)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model.output.weight, model.output.bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdl_p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
