{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4\n",
    "\n",
    "### Introduction\n",
    "\n",
    "* Top down \"getting things done\" approach (as before).  \n",
    "\n",
    "* Details of the models (RNN / Transformers) will come later (Chater 12, Part 2)\n",
    "\n",
    "### Kaggle Notebook / Video Lecture\n",
    "\n",
    "*  Notebook: [Getting started with NLP for absolute beginners] (https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners) \n",
    "\n",
    "    * Based on the US Patent Phrase to Phrase matching competition.  \n",
    "\n",
    "    * Consists of 'anchors' 'context' 'targets' and 'scores'.  The anchor and target are the things we want to compare (in the context 'context'), and the score is the comparison (0,.25, .5, .75, 1.0) are the possibilities. \n",
    "\n",
    "    * Context is some code like `A47`, the CPC classification code, the subject within which the simularity is to be scored. \n",
    "\n",
    "    * The key trick is to turn this into a classification problem, that is to classify (regress) the following text :  \n",
    "\n",
    "   `df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor`\n",
    "\n",
    "    ```\n",
    "    0    TEXT1: A47; TEXT2: abatement of pollution; ANC...\n",
    "    1    TEXT1: A47; TEXT2: act of abating; ANC1: abate...\n",
    "    2    TEXT1: A47; TEXT2: active catalyst; ANC1: abat...\n",
    "    3    TEXT1: A47; TEXT2: eliminating process; ANC1: ...\n",
    "    4    TEXT1: A47; TEXT2: forest region; ANC1: abatement\n",
    "    ```\n",
    "\n",
    "    * NB:  I ran it with 'pin to original environment' on.\n",
    "\n",
    "* Note: Jeremy emphasizes that classification using NLP is very accessible and has wide application.  \n",
    "\n",
    "* Video and notebook have a discussion of some basics, which I skimmed:\n",
    "    * train / val / test split and why its needed and \n",
    "[How (and why) to create a good validation set](https://www.fast.ai/posts/2017-11-13-validation-sets.html). To make sure you don't overfit your hyperparameters / model choice on validation data, hold out test data until the very end! \n",
    "\n",
    "    * Pearson correlation coefficient, the metric to be used for the Kaggle competition. (N.B., I think his presentation would have been clearer if he standardized the variables first? $ \\beta = r \\frac{s_x}{s_y} $ )  \n",
    "    \n",
    "    * There is an aside in the video discussing [The problem with metrics is a big problem for AI](https://www.fast.ai/posts/2019-09-24-metrics.html#:~:text=The%20problem%20with%20metrics%20is%20a%20big%20problem,environments%20...%205%20When%20Metrics%20are%20Useful%20).  For example, metrics becoming targets (Goodhart's Law \"When a measure becomes a target, it ceases to be a good measure\"). Typical examples are KPP's and SPIF's having unintended consequences.  AI can run amok chasing metrics. \n",
    "\n",
    " \n",
    "* Language Model PreTraining: \n",
    "    *  Self-supervised learning: Training a model using labels that are embedded in the independent variable, rather than requiring external labels.  The labels are built into the data!  Only in step three do we need labeled data.  \n",
    "\n",
    "    * RNN's tend to use 'next word' prediction, while Tranformers used here (BERT type models) are trained on 'masked word' prediction.\n",
    "\n",
    "    * Next word prediction training (and masked training) can create language models with deep 'understanding' of language. \n",
    "\n",
    "    \n",
    "\n",
    "* UML Fit\n",
    "    * Again approach is to finetune a pre-trained model.  \n",
    "    \n",
    "    * Jeremy uses CNN as an example, that it is the later layers that are task specific. Specifically, in the image case, the last layer that was used in the pretraining for classification we throw away and add on a new classification head for a specific problem, ( random matrix)  , and train that. More on how to do this in detail from scratch in part two.\n",
    "\n",
    "    * [ULMfit](https://arxiv.org/abs/1801.06146) - used an RNN \n",
    "\n",
    "        - Step one - Use a language model pretrained on next word predictions on a large corpus (e.g. wikipedia) \n",
    "        - Step two - Fine tune on next word for IMDB movie review\n",
    "        - Step three - Fine tune model on movie sentiment with a classifier head.\n",
    "\n",
    "\n",
    "    * Note there are a lot of pretrained models on the [Huggingface model library](https://huggingface.co/models), and some might be trained on something close to what you want to do. In this case he is using a general use model (microsoft/deberta-v3-small) which was good starting point at that time.\n",
    "\n",
    "\n",
    "\n",
    "* Steps for a language model:\n",
    "\n",
    "    * Tokenization  (break up into tokens) \n",
    "        * Word based (uncommon) - seperate on spaces\n",
    "\n",
    "        * Subword tokens (common) - better especially for languages where spaces are a useful seperation of concepts in a sentence. Note also this can handle non-language sequences, like music or DNA\n",
    "\n",
    "        * character based - Fun to play with! [Shakespeare model](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "    * Numericalization: Create a vocab matching each unique token to a number, and convert tokens to numbers. (Neural net can only work on numbers!)\n",
    "\n",
    "    * Language model data loader creation: Generates next token target, shuffles training data, etc.\n",
    "\n",
    "    * Language model creation:  Transformer or RNN (as in the chapter).  For now think of as just another deep neural network except that it can handle arbtitray lists of numbers. \n",
    "\n",
    "\n",
    "*  Note that they use \"AutoModelForSequenceClassification\" using num_labels =1 , so they are doing a regression (using `MSELoss()`) on the label not a classification. But it seems to work well here.  \n",
    "\n",
    "* Key takeaways: You can get pretty good results using existing pretrained models \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addional notes from Video\n",
    "\n",
    "* Transformers were introduced around the time of the ULMfit. Advantages:\n",
    "   - Parallel training on GPUs\n",
    "   - No vansishing gradients\n",
    "\n",
    "* Jeremy says that transformers are not well suited to next word prediction (?? But they are! Gilbert 2018) and that they instead used masked word prediction (i.e. predict a missing word.) This is the case for many models, but also next word prediction is done.  Note that DeBerta is a variant of masked language model. \n",
    "\n",
    "\n",
    "#### Key libraries:\n",
    "\n",
    "* Numpy\n",
    "* Pandas\n",
    "* Matplotlib (Seaborn)\n",
    "* Pytorch (and others like sklearn and statsmodels)\n",
    "\n",
    "Recommends [Python for Data Analysis](https://wesmckinney.com/book/). The book covers mainly the first three, and touches on statsmodels and sklearn at the end.\n",
    "\n",
    "\n",
    "### Use and Misuse of NLP\n",
    "\n",
    "* NLP is moving fast, things are possible now that are not possible are year ago!! Huge oppurtunity area.\n",
    "\n",
    "* FCC Comments to 2017 proposal to repeal net neutrality had huge number (> 95%) of (likely) [fake pro-repeal](https://hackernoon.com/more-than-a-million-pro-repeal-net-neutrality-comments-were-likely-faked-e9f0e3ed36a6) comments. This may have effected the outcome!   with current tech this would be much harder to detect\n",
    "\n",
    "* Fake identiy Katie Jones on linked in.  (Fake images),  GPT-2 conversatinos with itself on reddit, etc.\n",
    "\n",
    "* Since the book and video were made, we all know that these models have gotten even better (SONA!) ! There will always be an arms race between the fake generates and the fake detectors.\n",
    "\n",
    "\n",
    "### Final question about categorical vs regression\n",
    "\n",
    "Jeremy mentions that yes, if you pass in num_labels = 1 you get a regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging face tasks\n",
    "\n",
    "[Hugging face tasks](https://huggingface.co/tasks) contains helpful starting points for a variety of tasks, including:\n",
    "\n",
    "* Text Classification (what we were doing here)\n",
    "\n",
    "* Question Answering (given a question and a text containing the answer, give the answer)\n",
    "\n",
    "* Text generation (Chat gpt)\n",
    "\n",
    "* .... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 10 of the book\n",
    "\n",
    "This seems to similar to the video with these differences:\n",
    "\n",
    " -  Uses the IMDB sentiment classification problem (as in the ULMFit paper). \n",
    " - Uses Fast.AI library instead of Hugginface library\n",
    " -  Uses RNN's instead of transformers \n",
    " \n",
    "Like the video, there is not much said about the structure of these models. (Ch 12 does it at a lower level, and part 2 of the course will also go deeper.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 10 extra notes \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "* Embedding \n",
    "   * After numericalization, the first layer maps each index into a n-dimensional 'embedding' vector.   This is mentioned in Chapter 10, but embedding in general is discussed in CHapter 9 as well as Lesson 7.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Fast AI tools \n",
    "\n",
    "   * Fastai has a general purpose tokenizers for word and subword tokenization.  (`SpacyTokenizer()` for words, `SentencePieceTokenizer` for subword.  The source fastai.text.core defines aliasa for these: `WordTokenizer` and `SubWordTokenizer` but they are *not* documented. ).  \n",
    "   \n",
    "   * Note that SentancePieceTokenizer must be trained: It creates a vocab of a speficied size by finding common sequences of characters.\n",
    "\n",
    "   * Not also that in displaying tokenized output an underscore is commonly used to represent spaces in the original text (so that spaces can be used to seperate tokens) \n",
    "\n",
    "   * Reminder that the transformer models all come packaged with the right tokenizer to use on those models.\n",
    "\n",
    "   * Special tokens can be used for things like \"Beginning of Stream\" etc. Fastai Tokenizer takes care of adding these special tokens.\n",
    "\n",
    "   * Fastai has also a 'Numericalize' to create numerical representation of the vocab\n",
    "\n",
    "   * In fast ai `DataBlock` also creates the 'dependant' variable by offseting the streams by one token. For text processing, we need to pass in TextBlock for the blocks arguments. Two ways to preprocess the text:\n",
    "   \n",
    "      - For pretraining, `is_lm=True` processes the text for next word prediction as follows:\n",
    "         * All the reviews are shuffled\n",
    "         * Concat together into one long stream\n",
    "         * Cut stream in a certain number of batches. If batch size was 64, we would now have 64 streams.  \n",
    "         * Model then reads the ministreams in order (slicing them up further in to sequences of `seq_len` to create fixed size tensors) \n",
    "      \n",
    "      - While `is_lm=False` is for classification / regression heads.  Since each review has a different length, and we need each tensor to be a single shape, we use padding to make them the same size. This uses a padding token that the model knows to ignore.  Further they roughly group texts that are close to the same size to minimize padding (each batch doesn't have to use the same padding!). This sorting and padding is down automatically in Fastai when using `TextBlock` with `is_lm = False` (the default).  Note this is not an issue with the pretraining, since we concat and then split into equal sized pieces! \n",
    " \n",
    "* Notes on fitting in fastai:\n",
    "\n",
    "   * `fit_one_cycle` will freeze the model except the random embeddings. My attempt at 'step 2' training on  Paperspace training would took 45 minutes .  Saved the model 353MB. Unfortunately I obliterated it when I reran the notebook and did not make another attempt at this.  It is a bit frustrating! Next step would have been to unfreeze and (pre-train) some more. \n",
    "\n",
    "   * Text generation: For fun he uses this model to generate random reviews. \n",
    "\n",
    "   * Final step: extract the pretrained 'encoder' (i.e. all but the last layer)  \n",
    "\n",
    "   * Create a new dataloader , using the labeled data , TextBlock and CategoryBlock. This adds a classification head. \n",
    "\n",
    "   * Finally he uses this (pretrained) model to train on classsifcation using a gradual unfreezing method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdl_p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
