{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4\n",
    "\n",
    "Like the rest of the course so far, we are taking a top down 'getting things done' approach, using models without digging into how they work.  RNN? Transformers?  That will come later.\n",
    "\n",
    "### Getting started with NLP for absolute beginners\n",
    "\n",
    "Recommended first doing the kaggle notebook [Getting started with NLP for absolute beginners] (https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners) \n",
    "\n",
    "  - I ran it with 'pin to original environment' on.\n",
    "\n",
    "   - It really is for beginners in that it goes through a lot of basics (like train/val/test split, and pearson correlation coefficient). I skimmed most of this and suspect most of the group will do so as well.  Note he says something not quite right about correlation and slope. $ \\beta = r \\frac{s_x}{s_y} $\n",
    "\n",
    "   - Based on the US Patent Phrase to Phrase matching competition. Uses classification in an interesting way to classify the combination of two phrases as similar, different or identical.\n",
    "\n",
    "   - The dataset consists of 'anchors' 'context' 'targets' and 'scores'.  The anchor and target are the things we want to compare (in the context 'context'), and the score is the comparison (0,.25, .5, .75, 1.0) are the possibilities.  The key trick is to turn this into a classification problem, that is to classify (regress) the following text :  \n",
    "\n",
    "   `df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor`\n",
    "\n",
    "   - Context is some code like `A47`, the CPC classification code, the subject within which the simularity is to be scored.   \n",
    "\n",
    "   - Tokenization splits the text into tokens (subword usually), and then turns the tokens into a number for each unique token (vocab). Why? Because neural nets can only use numbers! Note, you must use the right tokenizer for the model you are using. Notebook has good examples of how 'subword' looks.\n",
    "\n",
    "   - Huggingface dataset is different from pytorch dataset !  Huggingface models expect a Hugginface `dataset`, which can be created from a pandas dataframe.\n",
    "\n",
    "   - Note there are a lot of pretrained models on the [Huggingface model library](https://huggingface.co/models), and some might be trained on something close to what you want to do. In this case he is using a general use model (microsoft/deberta-v3-small) which was good starting point at that time.\n",
    "\n",
    "   - Note that they use \"AutoModelForSequenceClassification\" using num_labels =1 , so they are doing a regression (using `MSELoss()`) on the label not a classification. But it seems to work well here.  \n",
    "\n",
    "   - Key takeaways: You can get pretty good results using existing pretrained models \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video\n",
    "\n",
    "* This video doesn't use fast.ai library at all!  Just huggingface.  Huggingface is supposedly state of the art for NLP. \n",
    "\n",
    "* Video mentions that these advanced methods will be folded into fast.ai, this doesn't seem to be the case?\n",
    "\n",
    "[Official sumamry](https://course.fast.ai/Lessons/Summaries/lesson4.html)\n",
    "\n",
    "#### ULMFit \n",
    "\n",
    "* Again approach is to finetune a pre-trained model. First part of lecture tries to make what this means more clear using the slider model from previous chapter... imagine some sliders are already close.\n",
    "\n",
    "* [ULMfit](https://arxiv.org/abs/1801.06146) - used an RNN \n",
    "\n",
    "   - Step one - use a language model pretrained on next word predictions on wikipedia. \n",
    "   - Step two - Fine tune on next workd for IMDB movie review\n",
    "   - Step three - Fine tune model on movie sentiment with a classifier head.\n",
    "\n",
    "* Nice thing is that the pretraining requires no special labels, the labels are built into the data!  Only in step three do we need labeled data.\n",
    "\n",
    "* Transformers were introduced around the time of the ULMfit. Advantages:\n",
    "   - Parallel training on GPUs\n",
    "   - No vansishing gradients\n",
    "\n",
    "* Jeremy says that transformers are not well suited to next word prediction (?? But they are! Gilbert 2018) and that they instead used masked word prediction (i.e. predict a missing word.) This is the case for many models, but also next word prediction is done.  Note that DeBerta is a variant of masked language model. \n",
    "\n",
    "* Jeremy uses CNN as an example, that it is the later layers that are task specific. Specifically, in the image case, the last layer that was used in the pretraining for classification we throw away and add on a new classification head for a specific problem, ( random matrix)  , and train that. More on how to do this in detail from scratch in part two.\n",
    "\n",
    "#### Kaggle notebook\n",
    "\n",
    "* Next video walks through the notebook, see my notes above.  He assumes the same level for the audience: Not familiar with pandas, etc. \n",
    "\n",
    "* He emphasizes that classification using NLP is very accessible and has wide application. \n",
    "\n",
    "\n",
    "#### Key libraries:\n",
    "\n",
    "* Numpy\n",
    "* Pandas\n",
    "* Matplotlib (Seaborn)\n",
    "* Pytorch (and others like sklearn and statsmodels)\n",
    "\n",
    "Recommends [Python for Data Analysis](https://wesmckinney.com/book/). The book covers mainly the first three, and touches on statsmodels and sklearn at the end.\n",
    "\n",
    "#### Aside on validation sets and metrics\n",
    "\n",
    "[How (and why) to create a good validation set](https://www.fast.ai/posts/2017-11-13-validation-sets.html)\n",
    "\n",
    "Just about being careful, and make sure your validation set validates the task you really want to do. Key example is for time series, validation should be future points.\n",
    "\n",
    "Also one must be careful not to overfit to the validation set (through hyperparameter choice or model choice). Test set should be held out until the very end!\n",
    "\n",
    "[The problem with metrics is a big problem for AI](https://www.fast.ai/posts/2019-09-24-metrics.html#:~:text=The%20problem%20with%20metrics%20is%20a%20big%20problem,environments%20...%205%20When%20Metrics%20are%20Useful%20)\n",
    "\n",
    "* Issues with metrics becoming targets (Goodhart's Law \"When a measure becomes a target, it ceases to be a good measure\")  (e.g. KPPs, SPIF's etc have unintended consequences! )\n",
    "\n",
    "* AI makes this worse (Leverage)\n",
    "\n",
    "\n",
    "### Use and Misuse of NLP\n",
    "\n",
    "* NLP is moving fast, things are possible now that are not possible are year ago!! Huge oppurtunity area.\n",
    "\n",
    "* Fake comments and articles... could influence outcomes! \n",
    "\n",
    "\n",
    "### Final question about categorical vs regression\n",
    "\n",
    "Jeremy mentions that yes, if you pass in num_labels = 1 you get a regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging face tasks\n",
    "\n",
    "[Hugging face tasks](https://huggingface.co/tasks) contains helpful starting points for a variety of tasks, including:\n",
    "\n",
    "* Text Classification (what we were doing here)\n",
    "\n",
    "* Question Answering (given a question and a text containing the answer, give the answer)\n",
    "\n",
    "* Text generation (Chat gpt)\n",
    "\n",
    "* .... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 10 of the book\n",
    "\n",
    "This seems to similar to the \"Getting started with NLP\" notebook, except uses Fastai libraries and RNN's instead of Transformers.  And like the video, there is not much said about the structure of these models. (Ch 12 does it at a lower level, and part 2 of the course will also go deeper.)\n",
    "\n",
    "Some key take aways:\n",
    " \n",
    "* Self-supervised learning: Training a model using labels that are embedded in the independent variable, rather than requiring external labels. For instance, training a model to predict the next word in a text. \n",
    "\n",
    "\n",
    "* Next word prediction training (and masked training) can create language models with deep 'understanding' of language. \n",
    "\n",
    "* Universal Language Model Fine-tuning (UMLFit) is a three step process that improves transfer learning with pretrained models: \n",
    "\n",
    "    1.  Self-supervised train on large general corpus (e.g. wikipeidal). This part is already done for you with many models! (next word prediction)\n",
    "\n",
    "    2.  Self-supervised fine-tune on your data.  (next word prediction)\n",
    "\n",
    "    3.  Supervised train on your *labeled* data. (Classification)\n",
    "\n",
    "* Steps:\n",
    "\n",
    "    * Tokenization  (break up into tokens) \n",
    "        * Word based (uncommon) - seperate on spaces\n",
    "\n",
    "        * Subword tokens (common) - better especially for languages where spaces are a useful seperation of concepts in a sentence. Note also this can handle non-language sequences, like music or DNA\n",
    "\n",
    "        * character based - Fun to play with! [Shakespeare model](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "    * Numericalization: Create a vocab matching each unique token to a number, and convert tokens to numbers\n",
    "    * Language model data loader creation: Generates next token target, shuffles training data, etc.\n",
    "    * Language model creation:  Transformer or RNN (as in the chapter).  For now think of as just another deep neural network except that it can handle arbtitray lists of numbers. \n",
    "\n",
    "\n",
    "* Fastain (and Huggingspace) have tools to help you this.  I only skimmed the fast ai implementation in the book, where he uses the IMDb database with the goal of classifying review sentiment.  Some notes:\n",
    "\n",
    "   * Fastai has a general purpose tokenizers for word and subword tokenization.  (`SpacyTokenizer()` for words, `SentencePieceTokenizer` for subword.  The source fastai.text.core defines aliasa for these: `WordTokenizer` and `SubWordTokenizer` but they are *not* documented. ).  \n",
    "   \n",
    "   * Note that SentancePieceTokenizer must be trained: It creates a vocab of a speficied size by finding common sequences of characters.\n",
    "\n",
    "   * Not also that in displaying tokenized output an underscore is commonly used to represent spaces in the original text (so that spaces can be used to seperate tokens) \n",
    "\n",
    "   * Remind you that the transformer models all come packaged with the right tokenizer to use on those models.\n",
    "\n",
    "   * Special tokens can be used for things like \"Beginning of Stream\" etc. Fastai Tokenizer takes care of adding these special tokens.\n",
    "\n",
    "   * Fastai has also a 'Numericalize' to create numerical representation of the vocab\n",
    "\n",
    "* Text preprocessign was discussed in some detail (but this is all handled automatically by the libraries)\n",
    "   * All the reviews are shuffled\n",
    "   * Concat together into one long stream\n",
    "   * Cut stream in a certain number of batches. If batch size was 64, we would now have 64 streams.  \n",
    "   * Model then reads the ministreams in order (slicing them up further in to sequences of `seq_len` to create fixed size tensors)\n",
    "\n",
    "* in fast ai `Dataloader` also creates the 'dependant' variable by offseting the streams by one token.\n",
    "\n",
    "* All of the above is handled when TextBLock is passed to DataBlock in fastai. I guess it does word tokens by default?\n",
    "\n",
    "\n",
    "* Instead of transformer, this chapter uses a pretrained RNN  (`AWD-LSTM`) .  Promissed to show us how to do thsi from scratch in CHapter 12. \n",
    "\n",
    "    * Note important step that the model does: Embedds the word indices into a vector space. (Chapter 9 talks about this, and I presume we will later)\n",
    "\n",
    "    * What is NOT clear to me is how the pretrained vocab is supposed to match the one we created ? The text claims that the embeddings in the pretrained model are merged with random embeddings for words not in the pretrained vocab. But we never used the pretrained vocab at all as far as I can tell! However, looking at the docs it looks like the text learners  have a function (that is called behind the scenes?) that matches the embeddings to the pretrained model `match_embeddings`\n",
    "\n",
    "    * `fit_one_cycle` will freeze the model except the random embeddings. Paperspace training would took 45 minutes .  Saved the model 353MB. Unfortunately I obliterated it when I reran the notebook and did not make another attempt at this.  It is a bit frustrating!\n",
    "\n",
    "    * then we have to unfreeze and train some more.\n",
    "\n",
    "    * Then he saves the 'encoder' which is everything except the final (task specific) layer.\n",
    "\n",
    "* Text generation: For fun he uses the (full) model to generate random reviews. \n",
    "\n",
    "* Final step: using the pretrained 'encoder' to do classification\n",
    "\n",
    "   * Create a new dataloader , using the labeled data , TextBlock and CategoryBlock \n",
    "\n",
    "   * a wrinkle here each review has a different length, and we need each tensor to be a single shape. This is done by padding using a padding token that the model knows to ignore.  Further they roughly group texts that are close to the same size to minimize padding (each batch doesn't have to use the same padding!). This sorting and padding is down automatically in Fastai when using `TextBlock` with `is_lm = False` (the default).  Note this is not an issue with the pretraining, since we concat and then split into equal sized pieces! \n",
    "\n",
    "   * Finally he uses the pretrained model to train on classsifcation using a gradual unfreezing method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disinformation and Language Models\n",
    "\n",
    "This section was similar to the videos.   Examples:\n",
    "\n",
    "* FCC Comments to 2017 proposal to repeal net neutrality had huge number (> 95%) of (likely) [fake pro-repeal](https://hackernoon.com/more-than-a-million-pro-repeal-net-neutrality-comments-were-likely-faked-e9f0e3ed36a6) comments. This may have effected the outcome!   with current tech this would be much harder to detect\n",
    "\n",
    "* Fake identiy Katie Jones on linked in.  (Fake images),  GPT-2 conversatinos with itself on reddit, etc.\n",
    "\n",
    "* Since the book and video were made, we all know that these models have gotten even better (SONA!) ! There will always be an arms race between the fake generates and the fake detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdl_p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
