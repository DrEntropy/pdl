{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 21\n",
    "\n",
    "[Course Repo](https://github.com/fastai/course22p2)\n",
    "\n",
    "\n",
    "\n",
    "## WandB demo\n",
    "\n",
    "Jono: Demo of WandB.  (notebook 21_cifar10_and_wandb.ipynb) \n",
    "\n",
    "Main points:\n",
    "\n",
    "- Uses CFAR10 instead of fashion mnist.  The tools still work. \n",
    "- Weights and Biases (WandB) mointoring:\n",
    "    -  [WandB](https://wandb.ai/site) for monitoring and tracking of model performance. \n",
    "    -  Call logging functions in your code to log metrics, hyperparameters, and other data to the WandB server, and then you can view and compare these results in the WandB dashboard.\n",
    "    -  You can also use WandB to log models, code and more.\n",
    "- Jono implements a WandB callback in a few lines to implment this!\n",
    "- Jeremy says he doesn't use this (intentionally) because he worries about just doing wide sweeps of hyperparameters.  He prefers to do a more focused search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image quality metrics\n",
    "\n",
    "- We need a metric to evaluate the quality of the generated images, to compare different models.\n",
    "\n",
    "- Frechet inception distance (FID), demo in notebout 18_Fid.ipynb  Uses DDPM model from previous lesson. \n",
    "   - FID is a metric for generative models.\n",
    "   - It is a measure of how well the generated images are similar.\n",
    "   - Looks to see what the 'typical' final layer activations look like for a set of images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle,gzip,math,os,time,shutil,torch,random\n",
    "import fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "from scipy import linalg\n",
    "\n",
    "from fastcore.foundation import L\n",
    "import torchvision.transforms.functional as TF,torch.nn.functional as F\n",
    "from torch import tensor,nn,optim\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "from minai.datasets import *\n",
    "from minai.conv import *\n",
    "from minai.learner import *\n",
    "from minai.activations import *\n",
    "from minai.init import *\n",
    "from minai.sgd import *\n",
    "from minai.resnet import *\n",
    "from minai.augment import *\n",
    "from minai.accel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close\n",
    "from torch import distributions\n",
    "\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray_r'\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "set_seed(42)\n",
    "if fc.defaults.cpus>8: fc.defaults.cpus=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use existing classifier with classifier head removed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36539/39765531.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('models/data_aug.pkl')\n"
     ]
    }
   ],
   "source": [
    "xl,yl = 'image','label'\n",
    "name = \"fashion_mnist\"\n",
    "bs = 512\n",
    "\n",
    "@inplace\n",
    "def transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))*2-1 for o in b[xl]]\n",
    "\n",
    "dsd = load_dataset(name)\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)\n",
    "\n",
    "b = xb,yb = next(iter(dls.train))\n",
    "\n",
    "cbs = [DeviceCB(), MixedPrecision()]\n",
    "# I dont have a data_aug2 so hopefully this one will work\n",
    "model = torch.load('models/data_aug.pkl')\n",
    "learn = Learner(model, dls, F.cross_entropy, cbs=cbs, opt_func=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ResBlock(\n",
       "    (convs): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GeneralRelu()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (idconv): Sequential(\n",
       "      (0): Conv2d(1, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (act): GeneralRelu()\n",
       "  )\n",
       "  (1): ResBlock(\n",
       "    (convs): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GeneralRelu()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (idconv): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (act): GeneralRelu()\n",
       "  )\n",
       "  (2): ResBlock(\n",
       "    (convs): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GeneralRelu()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (idconv): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (act): GeneralRelu()\n",
       "  )\n",
       "  (3): ResBlock(\n",
       "    (convs): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GeneralRelu()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (idconv): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (act): GeneralRelu()\n",
       "  )\n",
       "  (4): ResBlock(\n",
       "    (convs): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GeneralRelu()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (idconv): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (act): GeneralRelu()\n",
       "  )\n",
       "  (5): ResBlock(\n",
       "    (convs): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GeneralRelu()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (idconv): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (act): GeneralRelu()\n",
       "  )\n",
       "  (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  (7): Linear(in_features=512, out_features=10, bias=False)\n",
       "  (8): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the output without the classifier head.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(learn.model[8])\n",
    "del(learn.model[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could also use callbacks to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ron/datadev/pdl/minai/accel.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  def before_fit(self, learn): self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 512]), tensor([9, 2, 1,  ..., 8, 1, 5]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats,y = learn.capture_preds()\n",
    "feats = feats.float()\n",
    "feats.shape,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flechet Inception Distance (FID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "NOTE: At 31:00 he discovers \"a bug\" in the images scaling.  \n",
    " \n",
    "- Uses means and covariance matrix of the global pooling layer (accross the samples/training data) of the classification model\n",
    "- These are compared for two images (generated and real) to get a distance metric.\n",
    "- Idea is that the pooling layer has activations for various feature in the images. If those features are correlated (e.g. ears and eyes) then the means and covariance matrix will be correlated.\n",
    " \n",
    "$$\n",
    "d_{F}(\\mathcal N(\\mu, \\Sigma), \\mathcal N(\\mu', \\Sigma'))^2 = \\lVert \\mu - \\mu' \\rVert^2_2 + \\operatorname{tr}\\left(\\Sigma + \\Sigma' -2\\left(\\Sigma \\Sigma'  \\right)^\\frac{1}{2} \\right)\n",
    "$$\n",
    "\n",
    "Note the need of the matrix square root. \n",
    "\n",
    "* Called Inception because the original paper used the Inception model.\n",
    "\n",
    "* Primary caveat is that you have to be careful comparing FID scores with different sized data sets. Not a universal metric.\n",
    "\n",
    "* FID is a good metric for comparing different models on the same data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need our generator again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betamin,betamax,n_steps = 0.0001,0.02,1000\n",
    "beta = torch.linspace(betamin, betamax, n_steps)\n",
    "alpha = 1.-beta\n",
    "alphabar = alpha.cumprod(dim=0)\n",
    "sigma = beta.sqrt()\n",
    "\n",
    "def noisify(x0, ᾱ):\n",
    "    device = x0.device\n",
    "    n = len(x0)\n",
    "    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n",
    "    ε = torch.randn(x0.shape, device=device)\n",
    "    ᾱ_t = ᾱ[t].reshape(-1, 1, 1, 1).to(device)\n",
    "    xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε\n",
    "    return (xt, t.to(device)), ε\n",
    "\n",
    "def collate_ddpm(b): return noisify(default_collate(b)[xl], alphabar)\n",
    "def dl_ddpm(ds): return DataLoader(ds, batch_size=bs, collate_fn=collate_ddpm, num_workers=4)\n",
    "\n",
    "dls2 = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n",
    "\n",
    "\n",
    "from diffusers import UNet2DModel\n",
    "\n",
    "class UNet(UNet2DModel):\n",
    "    def forward(self, x): return super().forward(*x).sample\n",
    "\n",
    "\n",
    "smodel = torch.load('models/fashion_ddpm_mp.pkl').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, sz, alpha, alphabar, sigma, n_steps):\n",
    "    device = next(model.parameters()).device\n",
    "    x_t = torch.randn(sz, device=device)\n",
    "    preds = []\n",
    "    for t in reversed(range(n_steps)):\n",
    "        t_batch = torch.full((x_t.shape[0],), t, device=device, dtype=torch.long)\n",
    "        z = (torch.randn(x_t.shape) if t > 0 else torch.zeros(x_t.shape)).to(device)\n",
    "        ᾱ_t1 = alphabar[t-1]  if t > 0 else torch.tensor(1)\n",
    "        b̄_t = 1 - alphabar[t]\n",
    "        b̄_t1 = 1 - ᾱ_t1\n",
    "        x_0_hat = ((x_t - b̄_t.sqrt() * model((x_t, t_batch)))/alphabar[t].sqrt())\n",
    "        x_t = x_0_hat * ᾱ_t1.sqrt()*(1-alpha[t])/b̄_t + x_t * alpha[t].sqrt()*b̄_t1/b̄_t + sigma[t]*z\n",
    "        preds.append(x_0_hat.cpu())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Show example for the case at hand, 512 channels. (around 38 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KID 50:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KID Kernal inception distance.   \n",
    "\n",
    "- Uses the features directly not the means accross the set. \n",
    "- \"The math doesnt matter\" \n",
    "- Measure of simularity between the distribtuions of the features.   \n",
    "- Has low bias but high variance.\n",
    "\n",
    "Jeramy creates a class for thsi that returns both FID and KID.  He shows for example how the images improve during the denoising process. KID and FID do look the same. \n",
    "\n",
    "Jeremy also looks at the 'real' fid using the real FID . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the scaling bug 1:00\n",
    "\n",
    "- Jeremy noticed that back in the DDPM_v2 notebook, the images were scaled from 0 to 1 instad of -1 to 1.   He fixes this:\n",
    "```\n",
    "@inplace\n",
    "def transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))*2-1 for o in b[xl]]\n",
    "```\n",
    "\n",
    "- This makes everything worse, so he spent two days trying to find the other bug that must be responsble and was being offset by this bug.\n",
    "\n",
    "- In the end he finds no relavent bug and asks: why is it a bug to scale from 0 to 1? Just because everyone else does it?  He tried it ny just subtracting 0.5 which keeps the range the same (smaller then normal). This leads to DDBM_v3. This is an improvement to the model, even accoring to the FID score.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedule experiments 1:09\n",
    "\n",
    "* uses 19 DDPM_v3 notebook.  \n",
    "\n",
    "* As part of his debugging he started to question everything, for example the $\\beta$ schedule\n",
    "\n",
    "* Tested Cosine squedule vs linear schedule to compare them.  Remember that $\\bar{\\alpha}$ is what really matters.  \n",
    "\n",
    "Note that the linear score has a long part of the time when the $\\bar{\\alpha}$ is near zero.  So he also tried decreasing $\\beta_{max}$. In fact the curves are similar in that case. So in the next version he changed $\\beta_{max}$ to 0.01   Results do look better. He also changed the model by making it bigger and training for longer.\n",
    "\n",
    "Fid is nearly as good as real image. \n",
    "\n",
    "\n",
    "* SKip sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[949, 959, 969, 979, 989, 999]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps  = 1000\n",
    "[t for t in range(n_steps) if (t+101)%((t+101)//100)==0][290:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def sample2(model, sz):\n",
    "    ps = next(model.parameters())\n",
    "    x_t = torch.randn(sz).to(ps)\n",
    "    sample_at = {t for t in range(n_steps) if (t+101)%((t+101)//100)==0}\n",
    "    preds = []\n",
    "    for t in reversed(range(n_steps)):\n",
    "        t_batch = torch.full((x_t.shape[0],), t, device=ps.device, dtype=torch.long)\n",
    "        z = (torch.randn(x_t.shape) if t > 0 else torch.zeros(x_t.shape)).to(ps)\n",
    "        ᾱ_t1 = alphabar[t-1]  if t > 0 else torch.tensor(1)\n",
    "        b̄_t = 1-alphabar[t]\n",
    "        b̄_t1 = 1-ᾱ_t1\n",
    "        if t in sample_at: noise = model((x_t, t_batch))\n",
    "        x_0_hat = ((x_t - b̄_t.sqrt() * noise)/alphabar[t].sqrt())\n",
    "        x_t = x_0_hat * ᾱ_t1.sqrt()*(1-alpha[t])/b̄_t + x_t * alpha[t].sqrt()*b̄_t1/b̄_t + sigma[t]*z\n",
    "        if t in sample_at: preds.append(x_t.float().cpu())\n",
    "    return preds\n",
    "\n",
    "    # And he has another that skipps even more less severly\n",
    "\n",
    "    # FID SCORE TILL pretty good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### 1:20  DDIM\n",
    "\n",
    "Whats the best paper for faster generation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
