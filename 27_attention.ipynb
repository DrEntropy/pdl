{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "273a6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math,torch\n",
    "from torch import nn\n",
    "from minai.activations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2c95260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e5ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models.attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43bd330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn(64,32,16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8174db82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = x.view(*x.shape[:2], -1).transpose(1, 2)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51245dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d9428",
   "metadata": {},
   "source": [
    "Three linear projections are needed. (I called these $W_q$ etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f10cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = nn.Linear(ni, ni)\n",
    "sq = nn.Linear(ni, ni)\n",
    "sv = nn.Linear(ni, ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15237a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = sk(t)\n",
    "q = sq(t)\n",
    "v = sv(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d34cd0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q@k.transpose(1,2)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ad9558",
   "metadata": {},
   "source": [
    "Group norm here, with number of groups = 1, standardizes over the channel dimension.  I believe this is the same as Layer norm in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ae3ec",
   "metadata": {},
   "source": [
    "Note that we set bias = False, as the Attention in Diffusers now turns this off by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15062786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)\n",
    "        self.norm = nn.GroupNorm(1, ni)\n",
    "        self.q = nn.Linear(ni, ni, bias=False)\n",
    "        self.k = nn.Linear(ni, ni, bias=False)\n",
    "        self.v = nn.Linear(ni, ni, bias=False)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        n,c,h,w = x.shape\n",
    "        x = self.norm(x)\n",
    "        x = x.view(n, c, -1).transpose(1, 2)\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = self.proj(x)\n",
    "        x = x.transpose(1,2).reshape(n,c,h,w)\n",
    "        return x + inp  # skip connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcb48706",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SelfAttention(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f509c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfAttention(\n",
       "  (norm): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "  (q): Linear(in_features=32, out_features=32, bias=False)\n",
       "  (k): Linear(in_features=32, out_features=32, bias=False)\n",
       "  (v): Linear(in_features=32, out_features=32, bias=False)\n",
       "  (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fb4ae6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra = sa(x)\n",
    "ra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce69830a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9386,  1.5361,  0.9011, -2.0526,  0.6612, -1.2371, -0.0379, -1.5735,\n",
       "        -0.7395,  1.6879, -0.3889, -1.4020, -0.7013, -0.5660, -0.7522,  0.7790],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1a380fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_parms(a,b):\n",
    "    b.weight = a.weight\n",
    "    b.bias = a.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc6e3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "at = Attention(32, heads= 1, dim_head = 32,out_dim = 32, residual_connection=1, norm_num_groups=1, dropout=0)\n",
    "src = sa.q, sa.k, sa.v, sa.proj, sa.norm\n",
    "dst = at.to_q, at.to_k, at.to_v, at.to_out[0], at.group_norm\n",
    "for s,d in zip(src,dst): cp_parms(s,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bfc0087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9386,  1.5361,  0.9011, -2.0526,  0.6612, -1.2371, -0.0379, -1.5735,\n",
       "        -0.7395,  1.6879, -0.3889, -1.4020, -0.7013, -0.5660, -0.7522,  0.7790],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at.eval()\n",
    "at(x)[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2bea6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9386,  1.5361,  0.9011, -2.0526,  0.6612, -1.2371, -0.0379, -1.5735,\n",
       "        -0.7395,  1.6879, -0.3889, -1.4020, -0.7013, -0.5660, -0.7522,  0.7790],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa(x)[0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5ae17",
   "metadata": {},
   "source": [
    "We could combine the three linear projections into one and chunk them out : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a4f25e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 96])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqkv = nn.Linear(ni, ni*3)\n",
    "st = sqkv(t)\n",
    "st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64df0786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q,k,v = torch.chunk(st, 3, dim=-1)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afdd291d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(k@q.transpose(1,2)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b54e9e",
   "metadata": {},
   "source": [
    "This version uses that qkv 'trick, and also replaces the group norm with batch norm.  Jermemy doesn't mention this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde31928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)\n",
    "        self.norm = nn.BatchNorm2d(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        n,c,h,w = inp.shape\n",
    "        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n",
    "        q,k,v = torch.chunk(self.qkv(x), 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = self.proj(x).transpose(1,2).reshape(n,c,h,w)\n",
    "        return x+inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1caa223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)\n",
    "        self.norm = nn.BatchNorm2d(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.norm(x).transpose(1, 2)\n",
    "        q,k,v = torch.chunk(self.qkv(x), 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        return self.proj(x).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67202ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = SelfAttention(32)\n",
    "sa(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf0c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0047, grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa(x).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de37a8",
   "metadata": {},
   "source": [
    "### Multihead Attention \n",
    "\n",
    "With multiheads, the model can attent to differnet parts of the image depending on the head, allowing it to attend to different parts at the same time. (Due to the softmax, the mixing tends to be quite peaky, so this is a good idea).\n",
    "\n",
    "This is done by just splitting the input features into small chunks, and running attention on each chunk.  The output is then concatenated.\n",
    "\n",
    "We apply the $W_q$ etc to the input, and then split, so that the split is not on the same features every time. \n",
    "\n",
    "To implement this, the just turn each of heads into a new batch item, run the attention as normal, and then split the batches back into heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a167b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heads_to_batch(x, heads):\n",
    "    n,sl,d = x.shape\n",
    "    x = x.reshape(n, sl, heads, -1)\n",
    "    return x.transpose(2, 1).reshape(n*heads,sl,-1)\n",
    "\n",
    "def batch_to_heads(x, heads):\n",
    "    n,sl,d = x.shape\n",
    "    x = x.reshape(-1, heads, sl, d)\n",
    "    return x.transpose(2, 1).reshape(-1,sl,d*heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11734bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c3466d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 256, 32]), torch.Size([512, 256, 4]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = rearrange(t , 'n s (h d) -> (n h) s d', h=8)\n",
    "t.shape, t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64105f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = rearrange(t2, '(n h) s d -> n s (h d)', h=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f34fabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 256, 4]), torch.Size([64, 256, 32]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape,t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4739d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t==t3).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ced6c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionMultiHead(nn.Module):\n",
    "    def __init__(self, ni, nheads):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.scale = math.sqrt(ni/nheads)\n",
    "        self.norm = nn.BatchNorm2d(ni) # note still using batchnorm \n",
    "        self.qkv = nn.Linear(ni, ni*3) # using the qkv thingy.\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        n,c,h,w = inp.shape\n",
    "        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n",
    "        x = self.qkv(x)\n",
    "        x = rearrange(x, 'n s (h d) -> (n h) s d', h=self.nheads)  ## heads_to_batch\n",
    "        q,k,v = torch.chunk(x, 3, dim=-1)  ## chunk the qkv projections\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = rearrange(x, '(n h) s d -> n s (h d)', h=self.nheads) ## batch_to_heads\n",
    "        x = self.proj(x).transpose(1,2).reshape(n,c,h,w)\n",
    "        return x+inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3ed8798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = SelfAttentionMultiHead(32, 4)\n",
    "sx = sa(x)\n",
    "sx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18c46b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0191, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0074, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx.mean(),sx.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd9b3f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "nm = nn.MultiheadAttention(32, num_heads=8, batch_first=True)\n",
    "nmx,nmw = nm(t,t,t)\n",
    "nmx = nmx+t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "451f2d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0021, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0040, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmx.mean(),nmx.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078e184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdl_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
