{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* last lesson of Part 2\n",
    "\n",
    "* CLiP embeddings will not be covered, but will be covered in the next Part (Which doesn't exist and probably never will).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion of pixels for sound. \n",
    "\n",
    "[Jono's Audio with Simple Diffusion](https://colab.research.google.com/drive/1b3CeZB2FfRGr5NPYDVvk34hyZFBtgub5?usp=sharing)\n",
    "\n",
    "* Jono said much of the code was copied from course notebooks, except he is applying it to sound. \n",
    "\n",
    "* Data is coming from a Hugging face dataset of bird calls.\n",
    "\n",
    "* Spectragram is a 2D representation of sound as images! \n",
    "\n",
    "* THe diffusers has Mel-spectragram function.  (This is a spectrogram using log scale for frequency i think)\n",
    "\n",
    "* Note the import has changed to `form diffusers import Mel`.   \n",
    "\n",
    "* The image conversion from the spectragram to audio is an approximation because the image has no phase information.\n",
    "\n",
    "* *Everything Else* is basically the same process of 'simple diffusion'([this paper maybe, Dec 2023](https://arxiv.org/abs/2301.11093)), using the model in [notebook 30](https://github.com/fastai/course22p2/blob/master/nbs/30_lsun_diffusion-simple.ipynb). This will be discussed i think later in this lesson??\n",
    "\n",
    "* Seems to work well.\n",
    "\n",
    "* There are people doing this with music as well: [Riffusion](https://www.riffusion.com/)  and also [SUno](https://suno.com/home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE \n",
    "At 17:50\n",
    "[Notebook 29](https://github.com/fastai/course22p2/blob/master/nbs/29_vae.ipynb)\n",
    "\n",
    "* The notebook starts with a simple auto encoder similar to what we did in the beginning of this course. \n",
    "\n",
    "* The issue is that altough the model can learn to encode and decode the data, it doesn't learn a good representation of the data. This is because it is only making a point estimate of the latent space.\n",
    "* The VAE solution is to learn an encoding with uncertainty,  into noise space, and then decode from noise space to the data space.\n",
    "* This is done by learning the mean and (log) variance in latent space, and then sampling from a normal distribution with that mean and variance for decoding. \n",
    "* To construct the loss function, we need to calculate the KL divergence between the learned distribution and a normal distribution wiht mean 0 and standard deviation 1. (The prior).  This is added to the standard cross entropy loss. \n",
    "* NB: This is the way it was presented in the video, but a better way to understand it is the entire loss is the KL divergence between the true latent posterior and this gaussian approximation to the posterior of the latents.  See for example [Derivaiton of VAE loss](https://arxiv.org/abs/1907.08956) \n",
    "\n",
    "Very roughly, if $z$ is latents adn $x$ is data, then the (backward) KL divergence is:\n",
    "\n",
    "$$\n",
    "D_{KL}(p_{\\theta}(z)||p(z|x)) = \\int p_{\\theta}(z) \\log \\frac{p_{\\theta}(z)}{p(z|x)} dz\n",
    "$$\n",
    "\n",
    "Apply Bayes rule, with prior $p(z)$ and likelihood $p(x|z)$, and evidence $p(x)$:\n",
    "\n",
    "$$\n",
    "p(z \\mid x) \\;=\\; \\frac{p(x \\mid z)\\,p(z)}{p(x)},\n",
    "$$\n",
    "\n",
    "We have: \n",
    "\n",
    " \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "D_{KL}(p_{\\theta}(z)\\,\\|\\,p(z \\mid x)) \n",
    "&= E_{z \\sim p_{\\theta}}\\!\\Biggl[\\log \\frac{p(x)\\, p_{\\theta}(z)}{p(z)\\,p(x\\mid z)}\\Biggr] \\\\\n",
    "&= E_{z \\sim p_{\\theta}}\\!\\Biggl[\\log \\frac{p_{\\theta}(z)}{p(z)} \\;-\\; \\log p(x\\mid z)\\Biggr] \\;+\\;\\log p(x) \\\\\n",
    "&= \\underbrace{E_{z \\sim p_{\\theta}}\\!\\Bigl[\\log \\frac{p_{\\theta}(z)}{p(z)}\\Bigr]}_{\\displaystyle D_{KL}\\bigl(p_{\\theta}(z)\\,\\|\\,p(z)\\bigr)} \n",
    "   \\;\\;-\\;\\;\\underbrace{E_{z \\sim p_{\\theta}}\\!\\bigl[\\log p(x\\mid z)\\bigr]}_{\\displaystyle \\text{NLL}} \n",
    "   \\;+\\;\\log p(x).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The evididence is not effeced by the model, so if we minimize the part that doesn't include $\\log p(x)$ we are minimizing the KL divergence.  This 'part' (or rather it's negative) is called the ELBO (evidence lower bound).\n",
    "\n",
    "This shows how the VAE loss is a combination of the KL divergence (between the latent distribution and the 'prior') and the NLL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSUN \n",
    "\n",
    "[Notebook 30](https://github.com/fastai/course22p2/blob/master/nbs/30_lsun_diffusion-latents.ipynb)\n",
    "42:40 \n",
    "\n",
    "* This introduces a bigger dataset (LSUN), in particular 256x256 color images of 'bedroom scenes'.\n",
    "\n",
    "* Jeremy uses a pretrained VAE from hugging face (https://huggingface.co/docs/diffusers/en/api/models/autoencoderkl) to encode the images into latents. \n",
    "\n",
    "* He shows how you can save these latents to numpy memory mapped files, so you can load them quickly later.\n",
    "\n",
    "* He then uses the diffusion model  we have developed to noisify and train a noise predictor on the latents themselves.  Now to generate images we just have to sample and decode.  The images look mostly pretty good. \n",
    "\n",
    "* I think at *this* point we have most of stable diffusion except of course the NLP part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full ImageNet \n",
    "[Notebook 31](https://github.com/fastai/course22p2/blob/master/nbs/31_imgnet_latents-widish.ipynb)\n",
    "\n",
    "This section uses the Kaggle ImageNet dataset, and trains a classifier using the latents. \n",
    "He does do some more pre-processing of the latents, including data augmentation.\n",
    "Idea is to use this model as a pretrained model for the noise predictor. \n",
    "He gets 66% accuracy (and there are 1000 categories!)\n",
    "\n",
    "This is a mission he gives the audience, to use this model as pretrained model (for the downsampling backbone) for the noise predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
