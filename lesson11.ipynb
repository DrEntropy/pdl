{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 11  - Matrix Multiplication \n",
    "\n",
    " \n",
    "## Recognizing students\n",
    "\n",
    "As before,  the video begins with a review of some student work, including some cool improvements to how the guidance parameter is determined. In one case, renomalizing hte parameter seemed to help the model produce more detailed images, and in another case a similar result was obtained by reducing the guidance (using a cosine function) as the denoising proceeded, so that the last few steps had no guidance at all.\n",
    "\n",
    "## How to read a research paper\n",
    "\n",
    "THe next section discussed a recent (as of the video date) paper: [DiffEdit: Diffusion-based semantic image editing with mask guidance](https://arxiv.org/abs/2210.11427).   Jeremy used this as an oppurtunity to give some tips on 'how to read a research paper'. I skimmed most of this as it was quite elementary.  \n",
    "\n",
    "The paper itself discussed a cool way to provide textually guided edits to an image.  The novelty here was automatically generating a mask.  Once the mask was produced, the edit is created by using standard text guided diffusion, except at each step the masked off areas are replaced with an appropriately noised version of the original image. Pretty cool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the Foundations \n",
    "\n",
    "### Matrix multiplication \n",
    "\n",
    "[course notebooks](https://github.com/fastai/course22p2)\n",
    "\n",
    "As you recall we have now basic tensors and random numbers. Now we need to learn to multiply them together. (Matrix multiplication).  Much of this section is also elementary so I will just give broad strokes for those parts.\n",
    "\n",
    "* He does matrix multiplication as a triple for loop. In python it is very slow\n",
    "\n",
    "* He then speeds it up with numba. \n",
    "\n",
    "* This allows us to use torch matrix multiplication.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba as nb\n",
    "import torch\n",
    "from minai import mnist_load\n",
    "from pathlib import Path\n",
    "\n",
    "path_data = Path('data')\n",
    "x_train, y_train, x_valid, y_valid = mnist_load.load_data(path_data)\n",
    "x_train,y_train,x_valid,y_valid = map(torch.tensor, (x_train,y_train,x_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = torch.randn(784,10)\n",
    "biases = torch.randn(10)\n",
    "m1 = x_valid[:10]\n",
    "m2 = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 784]), torch.Size([784, 10]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.shape, m2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@nb.njit\n",
    "def dot(a,b):\n",
    "    res = 0.\n",
    "    for i in range(len(a)): res+=a[i]*b[i]\n",
    "    return res\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test compiling works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 181 ms, sys: 22.4 ms, total: 204 ms\n",
      "Wall time: 328 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dot(np.array([1,2,3]),np.array([4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 μs, sys: 0 ns, total: 19 μs\n",
      "Wall time: 20.3 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dot(np.array([1,2,3]),np.array([4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a,b):\n",
    "    (ar,ac),(br,bc) = a.shape,b.shape\n",
    "    c =np.zeros((ar, bc))\n",
    "    for i in range(ar):\n",
    "        for j in range(bc):\n",
    "            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_c(a, b):\n",
    "    (ar, ac), (br, bc) = a.shape, b.shape\n",
    "    c = np.zeros((ar, bc))\n",
    "    for i in range(ar):\n",
    "        a_i = a[i, :]  # Cache row i of a\n",
    "        for j in range(bc):\n",
    "            b_j = b[:,j]\n",
    "            sum_ij = 0.0\n",
    "            for k in range(ac):\n",
    "                sum_ij += a_i[k] * b_j[k]  # Access via cached a_i\n",
    "            c[i, j] = sum_ij  # Assign after accumulation\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.7 ms ± 710 μs per loop (mean ± std. dev. of 7 runs, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "m1np = m1.numpy()\n",
    "m2np  = m2.numpy()\n",
    "%timeit -n 2 matmul(m1np,m2np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.5 ms ± 555 μs per loop (mean ± std. dev. of 7 runs, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 2 matmul_c(m1np,m2np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so something in the range of 10-20 ms for pure python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faster version\n",
    "@nb.njit\n",
    "def matmul2(a,b):\n",
    "    (ar,ac),(br,bc) = a.shape,b.shape\n",
    "    c =np.zeros((ar, bc))\n",
    "    for i in range(ar):\n",
    "        for j in range(bc):\n",
    "            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.1 μs ± 1.87 μs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n"
     ]
    }
   ],
   "source": [
    "matmul2(m1.numpy(),m2.numpy());  # force compilation\n",
    "%timeit -n 50 matmul2(m1np,m2np);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is 300 times faster then the naive python version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jermey does a little excursion into APL again, and then shows how pytorch does elementwise addition and multiplication with ease. \n",
    "\n",
    "Then he discusses the Frobenius norm, which is the square root of the sum of the squares of the elements of a matrix. (I.e. the 2-norm of a matrix).\n",
    "\n",
    "$$\n",
    "\\|A\\|_F = \\sqrt{\\sum_{i,j} A_{i,j}^2} = \\|A\\|_2\n",
    "$$\n",
    "\n",
    "I don't know why he brought this up, he then just moves on after showing how to do it in APL. \n",
    "\n",
    "\n",
    "### Matrix multiplication elementwise multiplication\n",
    "\n",
    "Back to matrix multiplication.  We can use the elementwise multiplication to speed up our matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "710 μs ± 14.2 μs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n"
     ]
    }
   ],
   "source": [
    "def matmul(a,b):\n",
    "    (ar,ac),(br,bc) = a.shape,b.shape\n",
    "    c = torch.zeros(ar, bc)\n",
    "    for i in range(ar):\n",
    "        for j in range(bc): c[i,j] = (a[i,:] * b[:,j]).sum()\n",
    "    return c\n",
    "\n",
    "%timeit -n 50 matmul(m1, m2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is a bit faster??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 μs ± 37.1 μs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n"
     ]
    }
   ],
   "source": [
    "def matmul(a,b):\n",
    "    (ar,ac),(br,bc) = a.shape,b.shape\n",
    "    c = np.zeros((ar, bc))\n",
    "    for i in range(ar):\n",
    "        for j in range(bc): c[i,j] = (a[i,:] * b[:,j]).sum()\n",
    "    return c\n",
    "\n",
    "%timeit -n 50 matmul(m1np, m2np);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster then the triple loop, but not as fast as the numba compiled version. (I compiled the full triple loop. In the video he only compiled the inner loop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    "- Discussion of broadcasting. (for more see numpy documentation). \n",
    "- Jeremy says this also goes back to APL and even further back to a language called Yorick.\n",
    "- shows how broadcasting can be understood using `expand_as` in pytorch. In numpy you can do this with `broadcast_to` \n",
    "- Unsqueeze or indexing with `None` (np.newaxis) can be used to expand dimensions which is very useful for broadcasting. This just inserts a new dimension of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.tensor([1,2,3])\n",
    "c[None] # because a trailing colon can be omitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4,  6],\n",
       "        [ 5,  7,  9],\n",
       "        [ 8, 10, 12]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "m + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the same as m + c[None].  But if we want to replicated accross columns we can explicitly use `None` to broadcast the other way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  3,  4],\n",
       "        [ 6,  7,  8],\n",
       "        [10, 11, 12]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m + c[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 4, 6],\n",
       "        [3, 6, 9]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[None,:] * c[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the outer product!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting rules - see [Numpy documentation](https://numpy.org/doc/stable/user/basics.broadcasting.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplication with broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take just one image.  This corresponds to one row of the matrix m1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), torch.Size([784, 10]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit = m1[0]\n",
    "digit.shape, m2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we want to do is to elementwise multiply this row times every single column in m2. But wait!  We just showed how to do this with broadcasting! We just need to expand m1 in the right way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit[:,None].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will expand to 784 x 10 by duplicating the *row* ten times to match the shape of m2, and then they can be elementwise multiplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(digit[:,None]*m2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new matrix now just needs to be summed along the rows to get the corresponding row of the output matrix.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -2.3374,   7.6787,  -8.9315,  10.8259, -10.4613, -19.6848,   5.1487,\n",
       "          6.7578,  -6.1243,  20.6067])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(digit[:,None]*m2).sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this together we have, so that we now just have one loop over the rows of m1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.3 μs ± 3.62 μs per loop (mean ± std. dev. of 7 runs, 500 loops each)\n"
     ]
    }
   ],
   "source": [
    "def matmul(a,b):\n",
    "    (ar,ac),(br,bc) = a.shape,b.shape\n",
    "    c = torch.zeros(ar, bc)\n",
    "    for i in range(ar):\n",
    "#       c[i,j] = (a[i,:] * b[:,j]).sum()      # previous version\n",
    "        c[i]   = (a[i,:,None] * b).sum(dim=0) # broadcast version\n",
    "    return c\n",
    "%timeit -n 500 _=matmul(m1, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats almost as fast as our numba version, and we are still doing the outer loop in python! We can now do the whole data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 784 ms, sys: 0 ns, total: 784 ms\n",
      "Wall time: 443 ms\n"
     ]
    }
   ],
   "source": [
    "%time matmul(x_train, weights) + biases;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch matmul (Extra, not in the video, will be in the next lesson)\n",
    "Matrix multiplication is easy in pytorch.  You can use the `@` operator or the `matmul` function. And it is faster  (10x) even then our numba version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.56 μs ± 3.98 μs per loop (mean ± std. dev. of 7 runs, 500 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 500  _ = m1 @ m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.3 ms ± 3.54 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 5 _ = x_train @ weights + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do even better using the cuda version, but we will get to that in the next lesson"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdl_p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
