{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 10 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 15 minutes of the lecture are a showcase of student work and then a review / recap of how stable diffusion works. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recent Papers \n",
    "Then he talks about two paper that came out recently (relative to the lecture)\n",
    "[Progressive Distillation for Fast Sampling of Diffusion Models](https://arxiv.org/abs/2202.00512) [paper walkthrough](https://www.youtube.com/watch?v=ZXuK6IRJlnk) .\n",
    "This takes use from 60 steps of denoising to just a few using 'distillation' of the model (which is a form of transfer learning).\n",
    "\n",
    "The teacher model is our fully trained stable diffusion model.  Then a student model (a unnet) is trained to skip over steps prgressively.  \n",
    "\n",
    "Another paper from same group shows how to use distillation for guided diffusion models:\n",
    "[On Distillation of Guided Diffusion Models](https://arxiv.org/abs/2210.03142)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " [Imagic: Text-Based Real Image Editing with Diffusion Models](https://arxiv.org/abs/2210.09276)\n",
    "\n",
    " The idea here is that you give the model an image and a text prompt.  It uses the text prompt to modify the image. For example, a picture of a dog and the prompt \"A dog sitting\" will create a picture of the same dog, sitting.\n",
    "\n",
    " The method to make this work is:\n",
    "\n",
    " 1- Train the embedding of the text prompt to be close to the the image.   This finds an embedding that is close the image but also close to the text prompt.\n",
    "\n",
    " 2 - Fine tune the whole model, using the new embedding, to the image. \n",
    " \n",
    " 3 - Use the model to generate the new image, interpolating between the original text embedding and the new one.. \n",
    "\n",
    "\n",
    "And, there is also this paper [Intruct Pix to Pix](https://arxiv.org/abs/2211.09800) that does the same kind of thing, except instead of providing a label of the final image, you provide *instructions*.   (e.g. \"Add fireworks to the sky\"). This is a specially trained model, using a dataset of images and instructions that were created from stable diffusion and GPT models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdl_p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
