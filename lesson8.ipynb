{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 8 : Convolution networks\n",
    "\n",
    "### But FIRST - finish up Collaborative filtering!\n",
    "\n",
    "I included notes on the rest of Chapter 8  in the previous lesson because I had no idea he was going to continue on in this lesson. \n",
    "\n",
    "For reference he goes over:\n",
    "\n",
    "* Creating your own embedding layer\n",
    "\n",
    "* Intepreting embeddings and biases \n",
    "\n",
    "* Using fastai's `collab_learner` implementation\n",
    "\n",
    "* Embedding distances \n",
    "\n",
    "* Deep learning for collab filtering\n",
    "\n",
    "\n",
    "### Embeddings for NLP (30:30)\n",
    "\n",
    "* Discussion of how embeddings are used words, which are just a large categorical variable (the vocab)\n",
    "\n",
    "* The neural net only sees the embeddings, which it learns.\n",
    "\n",
    "\n",
    "### Embeddings for Tabular\n",
    "\n",
    "* As mentioned (i think?) before we can also turn any categorical into an embedding, for example those that often appear in tabular data. \n",
    "\n",
    "* This is done in chapter 9 in the book.\n",
    "\n",
    "* Guo, Cheng, and Felix Berkhahn. 2016. “Entity Embeddings of Categorical Variables.” arXiv. https://doi.org/10.48550/arXiv.1604.06737.\n",
    "\n",
    "* Guo et. al. also combined deep learning embeddings with boosted trees (i.e. also feeding the embeddings into decision tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNS\n",
    "\n",
    "Video at 44:30\n",
    "\n",
    "* Based partly on Chapter 13 in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
