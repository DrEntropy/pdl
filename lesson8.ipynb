{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 8 : Convolution networks\n",
    "\n",
    "### But FIRST - finish up Collaborative filtering!\n",
    "\n",
    "I included notes on the rest of Chapter 8  in the previous lesson because I had no idea he was going to continue on in this lesson. \n",
    "\n",
    "For reference he goes over:\n",
    "\n",
    "* Creating your own embedding layer\n",
    "\n",
    "* Intepreting embeddings and biases \n",
    "\n",
    "* Using fastai's `collab_learner` implementation\n",
    "\n",
    "* Embedding distances \n",
    "\n",
    "* Deep learning for collab filtering\n",
    "\n",
    "\n",
    "### Embeddings for NLP (30:30)\n",
    "\n",
    "* Discussion of how embeddings are used words, which are just a large categorical variable (the vocab)\n",
    "\n",
    "* The neural net only sees the embeddings, which it learns.\n",
    "\n",
    "\n",
    "### Embeddings for Tabular\n",
    "\n",
    "* As mentioned (i think?) before we can also turn any categorical into an embedding, for example those that often appear in tabular data. \n",
    "\n",
    "* This is done in chapter 9 in the book.\n",
    "\n",
    "* Guo, Cheng, and Felix Berkhahn. 2016. “Entity Embeddings of Categorical Variables.” arXiv. https://doi.org/10.48550/arXiv.1604.06737.\n",
    "\n",
    "* Guo et. al. also combined deep learning embeddings with boosted trees (i.e. also feeding the embeddings into decision tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNS\n",
    "\n",
    "Video at 44:30\n",
    "\n",
    "* Based partly on Chapter 13 in the book.\n",
    "\n",
    "\n",
    "#### Convolutions and pooling\n",
    "* First he presents convolutions in excel.  Mentions that MaxPooling is not as common and modern visual learners use stride > 1 (typically 2?) and average pooling at the end. \n",
    "\n",
    "* Fastai uses \"Concat pooling\" and concatenates the average and max pooling layers at the end.\n",
    "\n",
    "* Points out that a convolution layer is really just a matrix multiplication: \n",
    "   * Flatten the input into a vector\n",
    "   * Now the convolution can be written as a matrix multiplying the vector, except that the is constrained: Many of the weights are zero, and the rest are shared.\n",
    "   * This is shown in Chapter 13 of the book.\n",
    "\n",
    "\n",
    "####  Dropout layers\n",
    "\n",
    "* Shows how dropout works in excel, corrupting the activations randomly for each batch. \n",
    "\n",
    "* This is a regularization technique that helps prevent overfitting.\n",
    "\n",
    "* The idea is that a human can look at the corrupted image and still see what it was, and a neural net should be able to as well. This forces the model to the learn the representation and not just memorize the data (overfitting).\n",
    "\n",
    "\n",
    "#### Activation functions\n",
    "\n",
    "* Pretty much any nonlinearity can be used as an activation function.\n",
    "\n",
    "* ReLU is the most common these days as it is fast, but others are commonly used as well.. tanh, sigmoid, etc.\n",
    "\n",
    "\n",
    "\n",
    "## WHAT NOW? \n",
    "\n",
    "*  Write - Code, papers, blog posts, etc.\n",
    "\n",
    "* Help - Forums, StackOverflow, etc.\n",
    "\n",
    "* Gather - Book clubs, meetups, study groups\n",
    "\n",
    "* Build - Apps, Work Projects, Libraries\n",
    "\n",
    "* Go on to part too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 13\n",
    "\n",
    "* The book and the lecutre overlap quite a bit in concept.\n",
    "\n",
    "* The book however also gives some pytorch examples of setting up a CNN for MNIST\n",
    "\n",
    "* Although I have done this before, I think I will reproduce it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "\n",
    "\n",
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdl_p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
