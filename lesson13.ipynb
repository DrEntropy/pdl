{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 13\n",
    "\n",
    "Much of the begninning of the video is a review of the basics of the neural network (as matrix multiplications and nonlinear pointwise function).  In the beginning he uses a simple 2 layer network with 50 hidden layers and 1 output layer that predicts the digit.  The loss is then the MSE of the difference between the actual digit and the output of this last layer. This is a simplified model that is not correct, but it is a good starting point to understand the basics of the neural network. \n",
    "\n",
    "First we have to set things up again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\n",
    "from pathlib import Path\n",
    "from torch import tensor\n",
    "from fastcore.test import test_close\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
    "np.set_printoptions(precision=2, linewidth=125)\n",
    "\n",
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He then discusses that with the loss in hand, we need gradients to know how to adjust the weights. There is a long discussion giving some intuition of the chain rule, which I will not repeat here, as I am quite familier with calculus and the chain rule.\n",
    "The chain rule is important because we have a function (the prediction) that is a composition of other functions.\n",
    " \n",
    "\n",
    "The approach is to start at the end, take the derivative, and keep multiplying (chain rule!) by the derivative of the next function until we reach the beginning. This is the backpropagation algorithm in essence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how this works, we will set up this super simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,m = x_train.shape\n",
    "c = y_train.max()+1  # number of classes.\n",
    "n,m,c\n",
    "\n",
    "nh = 40  # hidden layer.\n",
    "w1 = torch.randn(m,nh)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh,1)\n",
    "b2 = torch.zeros(1)\n",
    "\n",
    "def lin(x, w, b): return x@w + b\n",
    "def relu(x): return x.clamp_min(0.)\n",
    "     \n",
    "# very basic model:\n",
    "def model(xb):\n",
    "    l1 = lin(xb, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    return lin(l2, w2, b2)\n",
    "\n",
    "# reminder, mse loss, which is NOT really suitable for classification.\n",
    "def mse(output, targ): return (output[:,0]-targ).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to compute the gradients using the chain rule. The video walks through this, and it is certainly worth doing.  The way i did it is to write out long hand the equations and work through the derivatives, keeping track of the jacobians as we go. This is a bit tedious, but it is a good exercise to understand the chain rule.  I don't document that here though.\n",
    "\n",
    "we save the gradients (jacobians) as properties of the variables. e.g. `v.g`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    # grad of matmul with respect to input\n",
    "    inp.g = out.g @ w.t()    # note w.t() is the transpose of w, same as w.T  for 2D tensors.\n",
    "    #w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    w.g = inp.T @ out.g  # this is the same as the above line, but more concise. \n",
    "    b.g = out.g.sum(0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward pass, we can't use model() here, as we need to keep track of intermediate values.\n",
    "    l1 = lin(inp, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w2, b2)\n",
    "    diff = out[:,0]-targ\n",
    "    loss = diff.pow(2).mean() # note we don't care about the value of the loss, just the gradient, so this line is not needed.\n",
    "    \n",
    "    # backward pass:\n",
    "    out.g = 2.*diff[:,None] / inp.shape[0]\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    l1.g = (l1>0).float() * l2.g\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using vscodes 'debug cell'  we can explore what is happening, looking at the gradients in each layer.  In the video he uses pdb directly using `pdb.set_trace()` to do this.  Using the vscode debugger is much nicer, when it works. I think it is a bit.. buggy. For example you have to make sure that you have executed the current version of the functions you will be stepping into, and I think you have to avoid any extra blank lines at the top of cells.. It is probably worth knowing the basics of pdb though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for testing against later\n",
    "def get_grad(x): return x.g.clone()\n",
    "chks = w1,w2,b1,b2,x_train\n",
    "grads = w1g,w2g,b1g,b2g,ig = tuple(map(get_grad, chks))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pytorch to check our results using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkgrad(x): return x.clone().requires_grad_(True)\n",
    "ptgrads = w12,w22,b12,b22,xt2 = tuple(map(mkgrad, chks))\n",
    "\n",
    "def forward(inp, targ):\n",
    "    l1 = lin(inp, w12, b12)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w22, b22)\n",
    "    return mse(out, targ)\n",
    "\n",
    "loss = forward(xt2, y_train)\n",
    "loss.backward()\n",
    "\n",
    "for a,b in zip(grads, ptgrads): test_close(a, b.grad, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN by scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to create our own version of the basic pytorch modules by refactoring the code above.  \n",
    "In the video he uses this as an oppurtnity to introduce __call__.  \n",
    "I skipped an intermediate refactor where each module has repeated code to save inputs and outputs.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()\n",
    "\n",
    "\n",
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        # save inputs and outputs for later.\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "\n",
    "    def forward(self): raise Exception('not implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args) # note the * in front of self.args, this is to unpack the list of arguments.\n",
    "    def bwd(self): raise Exception('not implemented')\n",
    "     \n",
    "\n",
    "class Relu(Module):\n",
    "    def forward(self, inp): return inp.clamp_min(0.)\n",
    "    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g\n",
    "     \n",
    "\n",
    "class Lin(Module):\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "    def forward(self, inp): return inp@self.w + self.b\n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)\n",
    "     \n",
    "\n",
    "class Mse(Module):\n",
    "    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n",
    "    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]\n",
    "     \n",
    "\n",
    "model = Model(w1, b1, w2, b2)\n",
    "     \n",
    "\n",
    "loss = model(x_train, y_train)\n",
    "     \n",
    "\n",
    "model.backward()\n",
    "     \n",
    "\n",
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch \n",
    "Or better, use autograd and nn.Module.   We dont need to define (but can!) backward, because it uses autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 41.11,   3.76,  14.97, -14.83, -22.24, -15.44,  62.55,  -4.69,   7.02,  -1.47,   1.37,  -4.08, -29.48,  -0.13,\n",
       "         -0.27,  -3.64,  -3.19,  -2.51, -41.20,   0.99,  -3.10,  35.40,  20.97,  14.38, -24.57,  16.74, -10.15,   8.52,\n",
       "          5.86,  29.04, -10.40,  -6.18,  16.78, -28.45,   5.12,  15.02,  -2.23,  -2.19, -13.74,   0.83])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.w = torch.randn(n_in,n_out).requires_grad_()\n",
    "        self.b = torch.zeros(n_out).requires_grad_()\n",
    "    def forward(self, inp): return inp@self.w + self.b\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [Linear(n_in,nh), nn.ReLU(), Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return F.mse_loss(x, targ[:,None])\n",
    "    \n",
    "model = Model(m, nh, 1)\n",
    "loss = model(x_train, y_train.to(torch.float))\n",
    "loss.backward()\n",
    "\n",
    "l0 = model.layers[0]\n",
    "l0.b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now that we have seen how it works, we can use the pytorch versions in the sequal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch training (notebook number 4)\n",
    "\n",
    "About 1:18 minutes in, he jumps to notebook number 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve loss, use cross entropy loss / softmax .  \n",
    "\n",
    "In video he works through how to get from this (using basic math of logs and exponentials):\n",
    "\n",
    "$$\n",
    "\n",
    "\\log \\text{softmax}(x)_i = \\log \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "$$\n",
    "\n",
    "\n",
    "to this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that logsumexp uses the trick of factorign out $e^{max(x)}$ to avoid numerical instability.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model without loss\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x\n",
    "    \n",
    "model = Model(m, nh, 10) # 10 classes.\n",
    "pred = model(x_train) # raw predictions (untrained)\n",
    "sm_pred = log_softmax(pred) # log softmax of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy loss is :\n",
    "\n",
    "$$\n",
    "\\text{cross entropy loss} = - \\sum_i y_i \\log \\hat{y}_i\n",
    "$$\n",
    "\n",
    "$\\hat{y}$'s here are probability computed from the softmax.  $y_i$ is just 0 or 1 depending on the prediction, so all this really does is look up the log of the probability (log of the softmax) for the correct prediction.  For a full batch we average over all the samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at this for a batch of just 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]  #predictions are just integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.34, -2.36, -2.39], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[[0,1,2], y_train[:3]]   #the log of the softmax of the correct predictions for the first 3 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.32, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nll(input, target): return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "nll(sm_pred, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch combines softmax and nll to make a single function called F.cross_entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.32, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(pred, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.14)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = F.cross_entropy\n",
    "bs = 64  # batch size\n",
    "\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 3  # how many epochs to train for\n",
    "\n",
    "# compute accuracy metric\n",
    "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()\n",
    "\n",
    "accuracy(pred, y_train) # random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f'epoch {epoch}')\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(i+bs, n))\n",
    "        xb, yb  = x_train[s], y_train[s]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, 'weight'):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias   -= l.bias.grad   * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias  .grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for just 3 epochs this is VERY slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.96)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(x_train)\n",
    "accuracy(pred, y_train) # random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next time: refactor this loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdl_p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
