{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 12\n",
    "\n",
    "As always, the course notebooks are [here](https://github.com/fastai/course22p2).\n",
    "\n",
    "## Clip Interogator\n",
    "\n",
    "The [CLIP Interogator](https://huggingface.co/spaces/pharmapsychotic/CLIP-Interrogator) tries to find a good prompt to create an image similar to that you provided. (Or rather a good caption).  \n",
    "\n",
    "* Doesn't really reproduce the image, and it really can't.  A function mapping an image to text cannot be invertable, it is many to one mapping.   (But stable diffusion gives an approximate inversion! )\n",
    "\n",
    "* CLIP interrogator uses [Blip](https://arxiv.org/abs/2201.12086) along with a  hardwired set of artists, images, styles, movements, etc. (See [source code](https://github.com/pharmapsychotic/clip-interrogator/tree/main/clip_interrogator)). It searches through these lists , combining them somehow with the Blip resutls, to find the best match, comparing the text encoding to the image encoding.  \n",
    "\n",
    "\n",
    "* We can uses these text prompts to get images of a similar nature to the one provided.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication continued\n",
    "\n",
    "Get back to where we got to before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from minai import mnist_load\n",
    "from pathlib import Path\n",
    "\n",
    "path_data = Path('data')\n",
    "x_train, y_train, x_valid, y_valid = mnist_load.load_data(path_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move these into tensors\n",
    "x_train,y_train,x_valid,y_valid = map(torch.tensor, (x_train,y_train,x_valid,y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = torch.randn(784,10)\n",
    "biases = torch.randn(10)\n",
    "m1 = x_valid[:10]\n",
    "m2 = weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive python version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.1 ms ± 409 μs per loop (mean ± std. dev. of 7 runs, 4 loops each)\n"
     ]
    }
   ],
   "source": [
    "def matmul_naive(a,b):\n",
    "    (ar,ac),(br,bc) = a.shape,b.shape\n",
    "    c =np.zeros((ar, bc))\n",
    "    for i in range(ar):\n",
    "        for j in range(bc):\n",
    "            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n",
    "    return c\n",
    "m1np = m1.numpy()\n",
    "m2np = m2.numpy()\n",
    "%timeit -n 4 _=matmul_naive(m1np,m2np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE* the version from the video did not first convert the tensors to numpy arrays. If you pass in tensors, it will be much slower because of the overhead of converting tensors to numpy arrays.  (500 ms!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastest so far is Broadcasting version of the multiplication.  Down to just one for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 μs ± 40.3 μs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# reminder of our fastest so far, usign broadcasting:\n",
    "\n",
    "def matmul(a,b):\n",
    "    (ar,ac),(br,bc) = a.shape,b.shape\n",
    "    c = torch.zeros(ar, bc)\n",
    "    for i in range(ar):\n",
    "#       c[i,j] = (a[i,:] * b[:,j]).sum()      # previous version\n",
    "        c[i]   = (a[i,:,None] * b).sum(dim=0) # broadcast version\n",
    "    return c\n",
    "matmul(m1, m2)\n",
    "%timeit -n 5 _=matmul(m1, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about 150 times faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the full matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 35s, sys: 0 ns, total: 1min 35s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "x_trainnp = x_train.numpy()\n",
    "weightsnp = weights.numpy()\n",
    "%time  _=matmul_naive(x_trainnp, weightsnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449 ms ± 7.26 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 5 _=matmul(x_train, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(60+35)/.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "speed up is about 200 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einstein Summation\n",
    "\n",
    "A compact representation for combining products and sums in a general way. The key rules are:\n",
    "\n",
    "  - Repeating letters between input arrays means that values along those axes will be multiplied together.\n",
    "\n",
    "  - Omitting a letter from the output means that values along that axis will be summed.\n",
    "\n",
    "  - Can be used to express many matrix manipulations: \n",
    "      - Transpose : `ij -> ji`  or just `ji`\n",
    "      - Inner Product : `i,i`\n",
    "      - Trace : `ii`\n",
    "      - Matrix multiplication: `ik,kj -> ij`  \n",
    "\n",
    "  - Invented by Albert Einstein in 1916 in his paper on the General Theory of Relativity. In GR there are many cases where (mathematical) tensors need to be summed over, so he invented a convention that repeated indexes should be summed over. This is standard now in physics when dealing with tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.53 ms ± 723 μs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n"
     ]
    }
   ],
   "source": [
    "# einssum version\n",
    "def matmul(a,b): return torch.einsum('ik,kj->ij', a, b)\n",
    "%timeit -n 50 _=matmul(x_train, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using native pytorch einsum, we can get another 100 x speed up! \n",
    "\n",
    "### Pytorch OP\n",
    "\n",
    "We can also express this using `matmul` or `@` in pytorch directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.27 ms ± 642 μs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 50 _ = x_train @ weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18:35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If cuda is available we can speed this up even more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.3 μs ± 806 ns per loop (mean ± std. dev. of 7 runs, 50 loops each)\n"
     ]
    }
   ],
   "source": [
    "# cuda version\n",
    "cuda0 = torch.device('cuda:0')\n",
    "weights = weights.to(cuda0)\n",
    "x_valid = x_train.to(cuda0)\n",
    "# force compile\n",
    "_ = matmul(x_valid, weights)\n",
    "%timeit -n 50 _=matmul(x_valid, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we get another order of 100x from using cuda. so that 1e6 times faster than naive python version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdl_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
